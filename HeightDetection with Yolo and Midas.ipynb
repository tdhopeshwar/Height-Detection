{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec2b1f58-a7c7-4049-bbd7-0c1d19f0c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, InterpolationMode\n",
    "from tensorflow.keras.models import load_model  # For loading your height detection model\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e77787bf-8891-4f94-90ae-e4065085797d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dell/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-10-9 Python-3.11.2 torch-2.4.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "Using cache found in C:\\Users\\dell/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dell/.cache\\torch\\hub\\rwightman_gen-efficientnet-pytorch_master\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File format not supported: filepath=\"E:/Anaconda/height_prediction_model.h5\". Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(\"E:/Anaconda/height_prediction_model.h5\", call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m midas\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load your pre-trained height detection model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m height_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mE:/Anaconda/height_prediction_model.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust the path to your model\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:206\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy H5 format files (`.h5` extension). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that the legacy SavedModel format is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported by `load_model()` in Keras 3. In \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder to reload a TensorFlow SavedModel as an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference-only layer in Keras 3, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keras.layers.TFSMLayer(\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, call_endpoint=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserving_default\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(note that your `call_endpoint` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File format not supported: filepath=\"E:/Anaconda/height_prediction_model.h5\". Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(\"E:/Anaconda/height_prediction_model.h5\", call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name)."
     ]
    }
   ],
   "source": [
    "# Load YOLOv5 model (pretrained)\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # YOLOv5 small model for object detection\n",
    "\n",
    "# Load MiDaS model for depth estimation\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\n",
    "midas.to('cpu').eval()\n",
    "\n",
    "# Load your pre-trained height detection model\n",
    "height_model = load_model('\"E:/Anaconda/height_prediction_model.h5\"')  # Adjust the path to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c38b639-e07a-4f7b-b3af-c8796f7c0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform for MiDaS input image\n",
    "transform = Compose([\n",
    "    Resize((384, 384), interpolation=InterpolationMode.BILINEAR),  # Resize to 384x384 as required by MiDaS\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# Function to estimate depth using MiDaS\n",
    "def estimate_depth(image):\n",
    "    # Convert OpenCV image (numpy array) to PIL image\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for PIL\n",
    "    \n",
    "    # Apply MiDaS depth estimation model\n",
    "    input_batch = transform(image_pil).unsqueeze(0)  # Prepare the image for the model\n",
    "    with torch.no_grad():\n",
    "        prediction = midas(input_batch)\n",
    "        prediction = torch.nn.functional.interpolate(\n",
    "            prediction.unsqueeze(1),\n",
    "            size=image.shape[:2],  # Resize depth map to match the original image size\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False\n",
    "        ).squeeze()\n",
    "    \n",
    "    depth_map = prediction.cpu().numpy()\n",
    "    return depth_map\n",
    "\n",
    "# Function to predict height using the pre-trained height detection model\n",
    "def predict_height(image):\n",
    "    resized_image = cv2.resize(image, (150, 150))  # Resize to the input size of the height model\n",
    "    image_array = resized_image / 255.0  # Normalize\n",
    "    image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "    predicted_height = height_model.predict(image_array)\n",
    "    return predicted_height[0][0]  # Return the predicted height\n",
    "\n",
    "# Access the laptop camera (0 is usually the built-in webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Use YOLOv5 to detect objects in the frame\n",
    "    results = yolo_model(frame)\n",
    "    \n",
    "    # Get bounding boxes of detected objects (person detection in this case)\n",
    "    detected_objects = results.pandas().xyxy[0]  # Get bounding box dataframe\n",
    "    \n",
    "    # Apply MiDaS depth estimation on the current frame\n",
    "    depth_map = estimate_depth(frame)\n",
    "    \n",
    "    for i, row in detected_objects.iterrows():\n",
    "        # Only focus on 'person' class\n",
    "        if row['name'] == 'person':\n",
    "            # Get the bounding box coordinates\n",
    "            x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])\n",
    "            \n",
    "            # Crop the person from the frame using the bounding box\n",
    "            person_image = frame[y1:y2, x1:x2]\n",
    "            \n",
    "            # Predict height using your height detection model\n",
    "            predicted_height = predict_height(person_image)\n",
    "            \n",
    "            # Focus on the lower-most pixel (feet area) of the bounding box\n",
    "            feet_depth = depth_map[y2, (x1 + x2) // 2]  # Take the center of the bounding box's bottom\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            \n",
    "            # Display the predicted height and depth\n",
    "            cv2.putText(frame, f\"Predicted Height: {predicted_height:.2f} inches\", (x1, y1 - 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "            cv2.putText(frame, f\"Depth: {feet_depth:.2f} meters\", (x1, y1 - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "    # Normalize depth map for visualization\n",
    "    depth_map_normalized = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    depth_map_normalized = np.uint8(depth_map_normalized)\n",
    "    \n",
    "    # Show the frame with YOLOv5 bounding boxes, predicted height, and depth map\n",
    "    cv2.imshow('YOLOv5 + MiDaS + Height Detection', frame)\n",
    "    cv2.imshow('Depth Map', depth_map_normalized)\n",
    "    \n",
    "    # Press 'q' to quit the video stream\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
